{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retina Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\내 드라이브\\\\Pytorch_Retinaface'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (391703770.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    cd G:/내 드라이브\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Retina Face train.py 폴더\n",
    "# cd G:/내 드라이브/Pytorch_Retinaface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!python train.py --network mobile0.25 --training_dataset ../dataset/wider_face/train/label.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\내 드라이브\\Pytorch_Retinaface\n"
     ]
    }
   ],
   "source": [
    "cd G:/내 드라이브/Pytorch_Retinaface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from models.retinaface import RetinaFace\n",
    "from data import cfg_mnet, cfg_re50\n",
    "\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "from utils.box_utils import decode, decode_landm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. HyperParmeter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- MobileNet 0.25 ---#\n",
    "weight_path = './weights/mobilenet0.25_Final.pth'\n",
    "cfg = cfg_mnet\n",
    "\n",
    "# #--- Resnet-50 ---#\n",
    "# weight_path = './weights/Resnet50_Final.pth'\n",
    "# cfg = cfg_re50\n",
    "\n",
    "resize = 1                      # image resize\n",
    "confidence_threshold = 0.02\n",
    "top_k = 5000\n",
    "nms_threshold = 0.4\n",
    "keep_top_k = 750\n",
    "vis_thres = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Model Loaded on 'cuda'!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "model = RetinaFace(cfg, phase = 'test').to(device)\n",
    "model.load_state_dict(torch.load(weight_path, map_location=device))     # map_location : 학습환경과 별개로 eval device 설정\n",
    "model.eval()\n",
    "print(f\"Model Loaded on '{device}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Retinaface Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retinaface_inf(test_img, model):\n",
    "    img = np.float32(test_img)\n",
    "    img_height, img_width, _ = img.shape\n",
    "\n",
    "    scale = torch.Tensor([img_width, img_height, img_width, img_height])\n",
    "    img -= (104, 117, 123)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    scale = scale.to(device)\n",
    "\n",
    "    tic = time.time()\n",
    "    loc, conf, landms = model(img)  # Forward-pass\n",
    "\n",
    "    # prior_data #\n",
    "    priorbox = PriorBox(cfg, image_size=(img_height, img_width))\n",
    "    prior_data = priorbox.forward().to(device).data\n",
    "    # boxes #\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    boxes = (boxes * scale / resize).cpu().numpy()\n",
    "    # scores #\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    # landms #\n",
    "    landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    scale1 = torch.Tensor([img.shape[3], img.shape[2],\n",
    "                           img.shape[3], img.shape[2], \n",
    "                           img.shape[3], img.shape[2], \n",
    "                           img.shape[3], img.shape[2],\n",
    "                           img.shape[3], img.shape[2]]).to(device)\n",
    "    \n",
    "    landms = (landms * scale1 / resize).cpu().numpy()\n",
    "\n",
    "    # Filter low score boxes\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    scores = scores[inds]\n",
    "    landms = landms[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    order = scores.argsort()[::-1][:top_k]\n",
    "    boxes = boxes[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    # NMS Processing\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    dets = dets[keep, :]\n",
    "\n",
    "    # Keep top-K faster NMS\n",
    "    dets = dets[:keep_top_k, :]\n",
    "\n",
    "    # dets = np.concatenate((data, landms), axis=1)\n",
    "    print('Net Forward time: {:.4f}'.format(time.time() - tic))\n",
    "    fps_ = round(1/(time.time() - tic), 2)\n",
    "    print('fps_ :', fps_)\n",
    "\n",
    "    # draw box\n",
    "    for b in dets:\n",
    "        if b[4] < vis_thres:\n",
    "            continue\n",
    "        text = \"{:.4f}\".format(b[4])\n",
    "        b = list(map(int, b))\n",
    "        cv2.rectangle(test_img, (b[0], b[1]), (b[2], b[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(test_img, text, (b[0], b[1]-6), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255), thickness=1, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    cv2.putText(test_img, \"retinaface\", (410, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (255,0,0), thickness=2, lineType=cv2.LINE_AA)\n",
    "    cv2.putText(test_img, \"fps : \"+str(fps_), (10, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return test_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "detect_num = 10\n",
    "itr = 0\n",
    "test_path = test_path = '../dataset/wider_face/test/images/0--Parade'\n",
    "for img_path in glob.glob(test_path+\"/*.jpg\"):\n",
    "    test_img = cv2.imread(img_path)\n",
    "    detect_img = result_retina = retinaface_inf(test_img, model)\n",
    "    plt.imshow(cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    itr += 1\n",
    "    if itr == detect_num: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. Dlib Inference 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "face_detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlib_inf(test_img, face_detector):\n",
    "    img = np.float32(test_img)\n",
    "    img_height, img_width, _ = img.shape\n",
    "\n",
    "    tic = time.time()\n",
    "    face_detection = face_detector(test_img)\n",
    "    fps_ = round(1/(time.time() - tic), 2)\n",
    "\n",
    "    for f in face_detection:\n",
    "        cv2.rectangle(test_img, (f.left(), f.top()), (f.right(), f.bottom()), (0,0,255), 4)\n",
    "    cv2.putText(test_img, \"dlib\", (410, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (255,0,0), thickness=2, lineType=cv2.LINE_AA)\n",
    "    cv2.putText(test_img, \"fps : \"+str(fps_), (10, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
